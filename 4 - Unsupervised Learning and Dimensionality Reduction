import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

# generate random dataset for demonstration
X, y = make_classification(
    n_samples = 500,
    n_features = 6,
    n_informative = 4,
    n_redundant = 2,
    random_state = 42
)

print(X.shape)

# display dataset of size (500, 6)

# standardization using StandardScaler()
x_scaled = StandardScaler().fit_transform(X)

# DIMENSIONALITY REDUCTION ----------------------------------------

# use Principal Component Analysis (PCA) as basis for dimension reduction
# assumes non-collinearity by transforming data into new features called principal components by using eigenvectors (directions) and eigenvalues (importance) from a covariance matrix.
# highest eigenvalues are selected and serves as basis for simplifying the dataset
# more variation = more useful information, so importance scales with variance alongside the direction that is most variable
# https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/
pca = PCA()
# fit data according to PCA
x_pca = pca.fit_transform(x_scaled)

# extract pca components and lay over into a dataframe
df_components = pd.DataFrame(
    pca.components_,
    columns = [f"PC{i + 1}" for i in range(X.shape[1])]
)
# pca.components_ has shape (n_components, n_features)
# columns here are how much each original feature contributes to PC
# therefore, if more accurate for future reference, columns should be X.columns and index should be PC{i + 1} for i in range(X.shape[1])
# need to confirm with someone to see if understood correctly

df_components

# display PCA matrix

# compute for cumulative sum of explained variance ratio in pca
# this will be the basis for determining how many features should be left
# explained_variance_ratio_ gives variance explained per PC
explained_var = np.cumsum(pca.explained_variance_ratio_)

# show elbow graph of number of features and their corresponding variance
plt.figure(figsize = (6,4))
plt.plot(range(1, len(explained_var)+1), explained_var, marker="o")
plt.grid(True)
plt.show()

# testing accuracy per n = PCs
# later to be used to explain total contributions to variance in dataset 
results = []
for n in range(1, X.shape[1] + 1):
  pca = PCA(n_components = n) # # of PCs per cycle
  x_reduced = pca.fit_transform(x_scaled) # fits dataset to reduced dimensions

  x_train, x_test, y_train, y_test = train_test_split(
      x_reduced, y, test_size = 0.3, random_state = 42
        )

  model = LogisticRegression()
  model.fit(x_train, y_train) # fit data splits to logistic regression

  predictions = model.predict(x_test) # use test inputs to make predictions
  score = accuracy_score(predictions, y_test) # compute trained model predictions to test
  results.append((n, score, pca.explained_variance_ratio_.sum()))
  # stores result per n cycle of score and explained variance ratios for trend representation

# transform results into dataframes with corresponding features aggregated in prior code block
df_acc = pd.DataFrame(results, columns=["Num PC", "score", "explained_var"])

plt.figure(figsize=(6,5))
plt.plot(df_acc["Num PC"], df_acc["score"], marker = "o", label = "Accuracy") # num of PC vs accuracy
plt.plot(df_acc["Num PC"], df_acc["explained_var"], marker="s", label = "Explained Var") # num of PC vs explained variance
plt.legend()
plt.grid(True)
plt.show()

# display multiple line graph of num PC against accuracy and explained variance
